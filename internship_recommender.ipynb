{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0aa29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "# import ollama\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import re\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 0. Resume Parser\n",
    "# # ==========================\n",
    "# import pdfplumber\n",
    "# import docx2txt\n",
    "\n",
    "# def parse_resume(file_path: str) -> str:\n",
    "#     ext = os.path.splitext(file_path)[1].lower()\n",
    "#     text = \"\"\n",
    "#     if ext == \".pdf\":\n",
    "#         with pdfplumber.open(file_path) as pdf:\n",
    "#             for page in pdf.pages:\n",
    "#                 text += page.extract_text() + \"\\n\"\n",
    "#     elif ext in [\".doc\", \".docx\"]:\n",
    "#         text = docx2txt.process(file_path)\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported file format. Please use PDF, DOC, or DOCX.\")\n",
    "#     # Clean text\n",
    "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "#     return text\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 1. Load Internship CSV\n",
    "# # ==========================\n",
    "# internships_df = pd.read_csv(\"internship_grok.csv\")  # your dataset\n",
    "\n",
    "# # Create a combined description for embedding\n",
    "# internships_df['description'] = (\n",
    "#     internships_df['internship_title'].astype(str) + \" at \" +\n",
    "#     internships_df['company_name'].astype(str) + \" | Location: \" +\n",
    "#     internships_df['location'].astype(str) + \" | Duration: \" +\n",
    "#     internships_df['duration'].astype(str) + \" | Stipend: \" +\n",
    "#     internships_df['stipend'].astype(str)\n",
    "# )\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 2. Candidate Resume Upload\n",
    "# # ==========================\n",
    "# resume_path = \"resume.pdf\"  # ðŸ“‚ put your resume file here\n",
    "# candidate_resume = parse_resume(resume_path)\n",
    "# print(\"ðŸ“„ Parsed Resume (first 300 chars):\", candidate_resume[:300], \"...\\n\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 3. Embedding Function (Local via Ollama)\n",
    "# # ==========================\n",
    "# class OllamaEmbeddingFunction:\n",
    "#     def __init__(self, model: str = \"mxbai-embed-large\"):\n",
    "#         self.model = model\n",
    "\n",
    "#     def __call__(self, input):\n",
    "#         if isinstance(input, str):\n",
    "#             input = [input]\n",
    "#         embeddings = []\n",
    "#         for t in input:\n",
    "#             response = ollama.embeddings(model=self.model, prompt=t)\n",
    "#             embeddings.append(response[\"embedding\"])\n",
    "#         return embeddings\n",
    "\n",
    "# embedding_fn = OllamaEmbeddingFunction(model=\"mxbai-embed-large\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 4. Persistent ChromaDB Setup\n",
    "# # ==========================\n",
    "# PERSIST_DIR = \"chroma_storage\"\n",
    "# chroma_client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "\n",
    "# # Use existing collection if already created\n",
    "# try:\n",
    "#     collection = chroma_client.get_collection(\n",
    "#         name=\"internships\", embedding_function=embedding_fn\n",
    "#     )\n",
    "#     print(\"ðŸ“‚ Loaded existing Chroma collection from disk\")\n",
    "# except:\n",
    "#     collection = chroma_client.create_collection(\n",
    "#         name=\"internships\", embedding_function=embedding_fn\n",
    "#     )\n",
    "#     print(\"ðŸ†• Created new Chroma collection\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 5. Chunking Function\n",
    "# # ==========================\n",
    "# def chunk_text(text, chunk_size=300, overlap=50):\n",
    "#     chunks = []\n",
    "#     start = 0\n",
    "#     while start < len(text):\n",
    "#         end = start + chunk_size\n",
    "#         chunk = text[start:end]\n",
    "#         chunks.append(chunk.strip())\n",
    "#         start += chunk_size - overlap\n",
    "#     return chunks\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 6. Chunk + Add Internship Descriptions (only if empty)\n",
    "# # ==========================\n",
    "# if collection.count() == 0:\n",
    "#     BATCH_SIZE = 500\n",
    "#     all_docs, all_ids = [], []\n",
    "\n",
    "#     for idx, desc in enumerate(internships_df['description'].tolist()):\n",
    "#         chunks = chunk_text(desc, chunk_size=300, overlap=50)\n",
    "#         for j, chunk in enumerate(chunks):\n",
    "#             all_docs.append(chunk)\n",
    "#             all_ids.append(f\"{idx}_{j}\")  # unique ID: internshipIndex_chunkIndex\n",
    "\n",
    "#     for i in range(0, len(all_docs), BATCH_SIZE):\n",
    "#         batch_docs = all_docs[i:i+BATCH_SIZE]\n",
    "#         batch_ids = all_ids[i:i+BATCH_SIZE]\n",
    "#         collection.add(documents=batch_docs, ids=batch_ids)\n",
    "#         print(f\"âœ… Inserted batch {i//BATCH_SIZE + 1} with {len(batch_docs)} chunks\")\n",
    "# else:\n",
    "#     print(f\"âš¡ Skipping insert: {collection.count()} chunks already stored\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 7. Chunk Candidate Resume\n",
    "# # ==========================\n",
    "# resume_chunks = chunk_text(candidate_resume, chunk_size=400, overlap=50)\n",
    "# print(f\"ðŸ“‘ Resume split into {len(resume_chunks)} chunks\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 8. Query Matches for Resume Chunks\n",
    "# # ==========================\n",
    "# top_n = 5\n",
    "# all_matches = []\n",
    "\n",
    "# for chunk in resume_chunks:\n",
    "#     results = collection.query(query_texts=[chunk], n_results=top_n)\n",
    "#     top_ids = results['ids'][0]\n",
    "#     top_texts = results['documents'][0]\n",
    "#     top_scores = results.get('distances', [[]])[0]\n",
    "\n",
    "#     raw_confidences = [1 - s for s in top_scores]\n",
    "#     min_c, max_c = min(raw_confidences), max(raw_confidences)\n",
    "#     normalized_confidences = [\n",
    "#         100 * (c - min_c) / (max_c - min_c + 1e-8) for c in raw_confidences\n",
    "#     ]\n",
    "\n",
    "#     for iid, text, conf in zip(top_ids, top_texts, normalized_confidences):\n",
    "#         all_matches.append((iid, text, conf))\n",
    "\n",
    "# # Deduplicate + sort by confidence\n",
    "# all_matches = sorted(all_matches, key=lambda x: x[2], reverse=True)\n",
    "# seen = set()\n",
    "# unique_matches = []\n",
    "# for iid, text, conf in all_matches:\n",
    "#     if iid not in seen:\n",
    "#         unique_matches.append((iid, text, conf))\n",
    "#         seen.add(iid)\n",
    "#     if len(unique_matches) >= top_n:\n",
    "#         break\n",
    "\n",
    "# print(\"\\nðŸ”Ž Top Internship Matches (with Confidence %):\")\n",
    "# for idx, (iid, text, conf) in enumerate(unique_matches, 1):\n",
    "#     print(f\"{idx}. ID {iid} | Confidence: {conf:.1f}%\")\n",
    "#     print(f\"   {text[:120]}...\\n\")\n",
    "\n",
    "# # ==========================\n",
    "# # ðŸ”¹ 9. RAG Explanation using LLaMA3.1\n",
    "# # ==========================\n",
    "# context = \"\\n\\n\".join([m[1] for m in unique_matches])\n",
    "# prompt = f\"\"\"\n",
    "# You are a resume-job matcher.\n",
    "# Context (Top {top_n} internship chunks):\n",
    "# {context}\n",
    "\n",
    "# Task:\n",
    "# Match the candidate's resume to the above internships and explain why the candidate is suitable.\n",
    "# Resume:\n",
    "# {candidate_resume}\n",
    "# \"\"\"\n",
    "\n",
    "# response = ollama.chat(\n",
    "#     model=\"llama3.1:8b\",\n",
    "#     messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "# )\n",
    "\n",
    "# print(\"\\nâœ… RAG Explanation:\\n\")\n",
    "# print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92193ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸ”¹ Imports\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "import chromadb\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2712a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# ðŸ”¹ 0. Resume Parser\n",
    "# ==========================\n",
    "def parse_resume(file_path: str) -> str:\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    text = \"\"\n",
    "    if ext == \".pdf\":\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    elif ext in [\".doc\", \".docx\"]:\n",
    "        text = docx2txt.process(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please use PDF, DOC, or DOCX.\")\n",
    "    # Clean text\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf8b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Parsed Resume (first 300 chars): Pooja Patnaik Address: Green Hills Road, Moosapet, Hyderabad Phone: 9110574275 E-Mail: patnaik.pooja9@gmail.com LinkedIn: www.linkedin.com/in/pooja-patnaik-74b367331 Profile As a final-year B. Tech student in CSE (AI & ML), I bring technical curiosity, disciplined learning, and hands-on project expe ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸ”¹ 1. Load Internship CSV (Modified for your dataset)\n",
    "# ==========================\n",
    "internships_df = pd.read_csv(\"internships_generated.csv\")  # ðŸ”¹ MODIFIED\n",
    "\n",
    "# Create a combined description for embedding (focus on relevant features: skills, location, qualification)\n",
    "internships_df['description'] = (\n",
    "    internships_df['Internship Title'].astype(str) + \" at \" +\n",
    "    internships_df['Company Name'].astype(str) + \" | Location: \" +\n",
    "    internships_df['Location'].astype(str) + \", \" +\n",
    "    internships_df['State/UT'].astype(str) + \" | Skills: \" +\n",
    "    internships_df['Preferred Skill(s)'].astype(str) + \" | Qualification: \" +\n",
    "    internships_df['Minimum Qualification'].astype(str)\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ 2. Candidate Resume Upload\n",
    "# ==========================\n",
    "resume_path = \"resume1.docx\"  # ðŸ“‚ put your resume file here\n",
    "candidate_resume = parse_resume(resume_path)\n",
    "print(\"ðŸ“„ Parsed Resume (first 300 chars):\", candidate_resume[:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ea7765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 21)\n",
      "Index(['Company Name', 'Internship Title', 'Sector', 'Area/Field',\n",
      "       'No. of Opportunities', 'Location', 'State/UT', 'District', 'Village',\n",
      "       'ZIP/Postal Code', 'Minimum Qualification', 'Course', 'Specialization',\n",
      "       'Certification(s)', 'Preferred Skill(s)', 'Qualification Description',\n",
      "       'Benefits', 'Benefits Description', 'Description',\n",
      "       'Candidates Already Applied', 'description'],\n",
      "      dtype='object')\n",
      "         Company Name               Internship Title          Sector  \\\n",
      "0  Adani Green Energy             Social Work Intern  NGO/Non-Profit   \n",
      "1       Bharti Airtel            Data Analyst Intern     Hospitality   \n",
      "2              Swiggy  Hospitality Operations Intern   Manufacturing   \n",
      "3     Microsoft India          Lab Technician Intern     Hospitality   \n",
      "4       Maruti Suzuki          Graphic Design Intern      Technology   \n",
      "\n",
      "                        Area/Field  No. of Opportunities Location  \\\n",
      "0                Digital Marketing                     3    Kochi   \n",
      "1                   Graphic Design                     8    Dausa   \n",
      "2                    Biotechnology                     7   Mysore   \n",
      "3  Journalism & Mass Communication                     7    Vizag   \n",
      "4                    Biotechnology                     5   Mysore   \n",
      "\n",
      "         State/UT       District     Village  ZIP/Postal Code  ...  \\\n",
      "0          Kerala      Ernakulam       Aluva           682001  ...   \n",
      "1       Rajasthan          Dausa      Lalsot           303511  ...   \n",
      "2       Karnataka         Mysore      Mandya           570001  ...   \n",
      "3  Andhra Pradesh  Visakhapatnam  Anakapalle           530001  ...   \n",
      "4       Karnataka         Mysore      Mandya           570001  ...   \n",
      "\n",
      "                   Course      Specialization  \\\n",
      "0                     BBA  Mass Communication   \n",
      "1                   B.Com       Biotechnology   \n",
      "2  Bachelor of Journalism       Biotechnology   \n",
      "3                     BBA       Biotechnology   \n",
      "4                     BBA    Hotel Management   \n",
      "\n",
      "                   Certification(s)            Preferred Skill(s)  \\\n",
      "0      Google Analytics Certificate        Excel, problem-solving   \n",
      "1            Lab Safety Certificate   Customer service, MS Office   \n",
      "2            Lab Safety Certificate  Event planning, coordination   \n",
      "3    Python Programming Certificate         MATLAB, data analysis   \n",
      "4  Communication Skills Certificate       Communication, teamwork   \n",
      "\n",
      "                           Qualification Description  \\\n",
      "0  Applicants with minimum qualification can appl...   \n",
      "1  Only candidates with specified qualification (...   \n",
      "2  Candidates meeting the minimum qualification (...   \n",
      "3  Candidates meeting the minimum qualification (...   \n",
      "4  Qualification eligibility as per minimum requi...   \n",
      "\n",
      "                                    Benefits  \\\n",
      "0  Stipend of â‚¹24,000; travel reimbursement.   \n",
      "1   Stipend of â‚¹7,000; travel reimbursement.   \n",
      "2  Stipend of â‚¹18,000; travel reimbursement.   \n",
      "3   Stipend of â‚¹9,000; travel reimbursement.   \n",
      "4  Stipend of â‚¹11,000; travel reimbursement.   \n",
      "\n",
      "                                Benefits Description  \\\n",
      "0       Mentorship program and soft skills training.   \n",
      "1  Professional development sessions and certific...   \n",
      "2  Skill-building workshops and career guidance s...   \n",
      "3  Hands-on training, mentorship, and networking ...   \n",
      "4  Skill-building workshops and career guidance s...   \n",
      "\n",
      "                                         Description  \\\n",
      "0  The Social Work Intern role involves assisting...   \n",
      "1  As a Data Analyst Intern, you will support gra...   \n",
      "2  Join as Hospitality Operations Intern in the B...   \n",
      "3  As a Lab Technician Intern, you will support j...   \n",
      "4  In this Graphic Design Intern, you will partic...   \n",
      "\n",
      "  Candidates Already Applied  \\\n",
      "0                         10   \n",
      "1                         18   \n",
      "2                          8   \n",
      "3                         16   \n",
      "4                         12   \n",
      "\n",
      "                                         description  \n",
      "0  Social Work Intern at Adani Green Energy | Loc...  \n",
      "1  Data Analyst Intern at Bharti Airtel | Locatio...  \n",
      "2  Hospitality Operations Intern at Swiggy | Loca...  \n",
      "3  Lab Technician Intern at Microsoft India | Loc...  \n",
      "4  Graphic Design Intern at Maruti Suzuki | Locat...  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "print(internships_df.shape)\n",
    "print(internships_df.columns)\n",
    "print(internships_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f398fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ†• Created new Chroma collection\n",
      "âœ… Inserted batch 1 with 200 chunks\n",
      "ðŸ“‘ Resume split into 15 chunks\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸ”¹ 3. Embedding Function (Local via Ollama)\n",
    "# ==========================\n",
    "class OllamaEmbeddingFunction:\n",
    "    def __init__(self, model: str = \"mxbai-embed-large\"):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, input):\n",
    "        if isinstance(input, str):\n",
    "            input = [input]\n",
    "        embeddings = []\n",
    "        for t in input:\n",
    "            response = ollama.embeddings(model=self.model, prompt=t)\n",
    "            embeddings.append(response[\"embedding\"])\n",
    "        return embeddings\n",
    "\n",
    "embedding_fn = OllamaEmbeddingFunction(model=\"mxbai-embed-large\")\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ 4. Persistent ChromaDB Setup\n",
    "# ==========================\n",
    "PERSIST_DIR = \"chroma_storage\"\n",
    "chroma_client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "\n",
    "# Use existing collection if already created\n",
    "try:\n",
    "    collection = chroma_client.get_collection(\n",
    "        name=\"internships\", embedding_function=embedding_fn\n",
    "    )\n",
    "    print(\"ðŸ“‚ Loaded existing Chroma collection from disk\")\n",
    "except:\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=\"internships\", embedding_function=embedding_fn\n",
    "    )\n",
    "    print(\"ðŸ†• Created new Chroma collection\")\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ 5. Chunking Function\n",
    "# ==========================\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk.strip())\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ 6. Chunk + Add Internship Descriptions\n",
    "# ==========================\n",
    "if collection.count() == 0:\n",
    "    BATCH_SIZE = 500\n",
    "    all_docs, all_ids = [], []\n",
    "\n",
    "    for idx, desc in enumerate(internships_df['description'].tolist()):\n",
    "        chunks = chunk_text(desc, chunk_size=300, overlap=50)\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            all_docs.append(chunk)\n",
    "            all_ids.append(f\"{idx}_{j}\")\n",
    "\n",
    "    for i in range(0, len(all_docs), BATCH_SIZE):\n",
    "        batch_docs = all_docs[i:i+BATCH_SIZE]\n",
    "        batch_ids = all_ids[i:i+BATCH_SIZE]\n",
    "        collection.add(documents=batch_docs, ids=batch_ids)\n",
    "        print(f\"âœ… Inserted batch {i//BATCH_SIZE + 1} with {len(batch_docs)} chunks\")\n",
    "else:\n",
    "    print(f\"âš¡ Skipping insert: {collection.count()} chunks already stored\")\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ 7. Chunk Candidate Resume\n",
    "# ==========================\n",
    "resume_chunks = chunk_text(candidate_resume, chunk_size=400, overlap=50)\n",
    "print(f\"ðŸ“‘ Resume split into {len(resume_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e31acf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Top Internship Matches (with Confidence %):\n",
      "1. ID 114_0 | Confidence: 100.0%\n",
      "   Data Analyst Intern at Swiggy | Location: Dausa, Rajasthan | Skills: AutoCAD, design | Qualification: Undergraduate...\n",
      "\n",
      "2. ID 12_0 | Confidence: 100.0%\n",
      "   Software Development Intern at Myntra | Location: Dehradun, Uttarakhand | Skills: Excel, problem-solving | Qualification...\n",
      "\n",
      "3. ID 58_0 | Confidence: 100.0%\n",
      "   Content Writing Intern at NTPC | Location: Mysore, Karnataka | Skills: AutoCAD, design | Qualification: Diploma...\n",
      "\n",
      "4. ID 26_0 | Confidence: 90.5%\n",
      "   Software Development Intern at Mindtree | Location: Indore, Madhya Pradesh | Skills: Excel, problem-solving | Qualificat...\n",
      "\n",
      "5. ID 67_0 | Confidence: 56.5%\n",
      "   Finance Analyst Intern at Swiggy | Location: Mysore, Karnataka | Skills: Excel, problem-solving | Qualification: Diploma...\n",
      "\n",
      "\n",
      "âœ… RAG Explanation:\n",
      "\n",
      "Based on the provided internships and Pooja Patnaik's resume, here are my matching suggestions:\n",
      "\n",
      "**Match 1:** Data Analyst Intern at Swiggy | Location: Dausa, Rajasthan | Skills: AutoCAD, design | Qualification: Undergraduate\n",
      "\n",
      "Pooja's relevant skills for this internship include AutoCAD, which is mentioned in her \"Projects\" section under AcuSym â€“ Symptom-Based Diagnosis Advisor. However, her experience and skills don't perfectly match the requirements of this internship.\n",
      "\n",
      "**Match 2:** Software Development Intern at Myntra | Location: Dehradun, Uttarakhand | Skills: Excel, problem-solving | Qualification: Undergraduate\n",
      "\n",
      "Pooja's relevant skills for this internship include Excel (mentioned in her \"Technical Skills\" section), which matches the requirements of this internship. However, her experience and skills are more inclined towards AI/ML and software development rather than traditional programming.\n",
      "\n",
      "**Match 3:** Content Writing Intern at NTPC | Location: Mysore, Karnataka | Skills: AutoCAD, design | Qualification: Diploma\n",
      "\n",
      "Pooja's relevant skill for this internship is AutoCAD (mentioned in her \"Projects\" section under AcuSym â€“ Symptom-Based Diagnosis Advisor). However, her experience and skills don't perfectly match the requirements of this internship.\n",
      "\n",
      "**Match 4:** Software Development Intern at Mindtree | Location: Indore, Madhya Pradesh | Skills: Excel, problem-solving | Qualification: Undergraduate\n",
      "\n",
      "Pooja's relevant skills for this internship include Excel (mentioned in her \"Technical Skills\" section), which matches the requirements of this internship. Additionally, her experience with AI/ML and software development makes her a strong candidate for this role.\n",
      "\n",
      "**Match 5:** Finance Analyst Intern at Swiggy | Location: Mysore, Karnataka | Skills: Excel, problem-solving | Qualification: Diploma\n",
      "\n",
      "Pooja's relevant skills for this internship include Excel (mentioned in her \"Technical Skills\" section), which matches the requirements of this internship. However, her experience and skills are more inclined towards AI/ML and software development rather than traditional finance analysis.\n",
      "\n",
      "Considering Pooja's skills and experience, I would recommend her for **Match 4:** Software Development Intern at Mindtree | Location: Indore, Madhya Pradesh. Her expertise in AI/ML, software development, and problem-solving make her a strong fit for this role.\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# ðŸ”¹ 8. Query Matches for Resume Chunks (Skills & Location Focus)\n",
    "# ==========================\n",
    "top_n = 5\n",
    "all_matches = []\n",
    "\n",
    "for chunk in resume_chunks:\n",
    "    results = collection.query(query_texts=[chunk], n_results=top_n)\n",
    "    top_ids = results['ids'][0]\n",
    "    top_texts = results['documents'][0]\n",
    "    top_scores = results.get('distances', [[]])[0]\n",
    "\n",
    "    raw_confidences = [1 - s for s in top_scores]\n",
    "    min_c, max_c = min(raw_confidences), max(raw_confidences)\n",
    "    normalized_confidences = [\n",
    "        100 * (c - min_c) / (max_c - min_c + 1e-8) for c in raw_confidences\n",
    "    ]\n",
    "\n",
    "    for iid, text, conf in zip(top_ids, top_texts, normalized_confidences):\n",
    "        # ðŸ”¹ Filter based on skills and location match (reduce bias)\n",
    "        if any(skill.lower() in candidate_resume.lower() for skill in text.split(\"Skills:\")[-1].split(\"|\")[0].split(\",\")) \\\n",
    "           or any(loc.lower() in candidate_resume.lower() for loc in text.split(\"Location:\")[-1].split(\"|\")[0].split(\",\")):\n",
    "            all_matches.append((iid, text, conf))\n",
    "\n",
    "# Deduplicate + sort by confidence\n",
    "all_matches = sorted(all_matches, key=lambda x: x[2], reverse=True)\n",
    "seen = set()\n",
    "unique_matches = []\n",
    "for iid, text, conf in all_matches:\n",
    "    if iid not in seen:\n",
    "        unique_matches.append((iid, text, conf))\n",
    "        seen.add(iid)\n",
    "    if len(unique_matches) >= top_n:\n",
    "        break\n",
    "\n",
    "print(\"\\nðŸ”Ž Top Internship Matches (with Confidence %):\")\n",
    "for idx, (iid, text, conf) in enumerate(unique_matches, 1):\n",
    "    print(f\"{idx}. ID {iid} | Confidence: {conf:.1f}%\")\n",
    "    print(f\"   {text[:120]}...\\n\")\n",
    "\n",
    "# ==========================\n",
    "# ðŸ”¹ 9. RAG Explanation using LLaMA3.1\n",
    "# ==========================\n",
    "context = \"\\n\\n\".join([m[1] for m in unique_matches])\n",
    "prompt = f\"\"\"\n",
    "You are a resume-job matcher.\n",
    "Context (Top {top_n} internship chunks):\n",
    "{context}\n",
    "\n",
    "Task:\n",
    "Match the candidate's resume to the above internships and explain why the candidate is suitable.\n",
    "Resume:\n",
    "{candidate_resume}\n",
    "\"\"\"\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=\"llama3.1:8b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… RAG Explanation:\\n\")\n",
    "print(response['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
